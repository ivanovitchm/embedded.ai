{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONNSVdh1JhLGeTkDuXlObq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"FdZ_Y3GYK2rX"},"source":["# AlexNet"]},{"cell_type":"markdown","metadata":{"id":"F81wxQ7lK6I7"},"source":["The work that perhaps could be credited with sparking renewed interest in neural networks and the beginning of the dominance of deep learning in many computer vision applications was the 2012 paper by Alex Krizhevsky et al. titled [ImageNet Classification with Deep Convolutional\n","Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). The paper describes a model later referred to as **AlexNet** designed to address the ImageNet Large Scale Visual Recognition Challenge or ILSVRC-2010 competition for classifying photographs of objects into one of 1,000 different categories.\n","\n","The ILSVRC was a competition designed to spur innovation in the field of computer vision. Before the development of AlexNet, the task was thought very difficult and far beyond the capability of modern computer vision methods. \n","\n","> AlexNet successfully demonstrated the capability\n","of the convolutional neural network model in the domain and kindled a fire that resulted in many more improvements and innovations, many demonstrated on the same ILSVRC task in subsequent years. \n","\n","More broadly, **the paper showed that it is possible to develop deep and effective end-to-end models** for a challenging problem without using unsupervised pre-training techniques popular at the time.\n","\n","Important in the design of AlexNet was a suite of new or successful methods, but not widely adopted at the time. Now, they have become requirements when using CNNs for image classification. \n","\n","> AlexNet used the rectified linear activation function, or ReLU, as the nonlinearly after each convolutional layer, instead of S-shaped functions such as the logistic or Tanh that were common up until that point. A softmax activation function was used in the output layer, now a staple for multiclass classification with neural networks.\n","\n","The average pooling used in LeNet-5 was replaced with a max-pooling method, although in this case, overlapping pooling was found to outperform non-overlapping pooling that is commonly used today (e.g., stride of pooling operation is the same size as the pooling operation,\n","e.g., 2 by 2 pixels). The newly proposed dropout method was used to address overfitting between the fully connected layers of the classifier part of the model to improve generalization error. The architecture of AlexNet is deep and extends upon some of the patterns established\n","with LeNet-5. The image below, taken from the paper, summarizes the model architecture, in this case, split into two pipelines to train on the GPU hardware of the time.\n","\n","<img width=\"800\" src=\"https://drive.google.com/uc?export=view&id=111DrLxQn-ejJ2-7zNA4M8t78wmqcMQNe\"/>\n","\n","\n","**It is similar to LeNet-5, only much larger and deeper**, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. Table below presents this architecture.\n","\n","<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=193aOD83q_m_apxqjv1kFSjGRYFV7HfL2\"></center><center>AlexNet Architecture.</center>\n","\n","\n","The model has **five convolutional layers** in the **feature extraction part** of the model and **three fully connected layers** in the **classifier part** of the model. Input images were fixed to the size **227x227 (there is a typo in the original paper using 224 x 224) with three color channels**. In terms of the number of **filters** used in each convolutional layer, the pattern of increasing the number of filters with depth seen in LeNet was mainly adhered to; in this case, the sizes: **96, 256, 384, 384, and 256**. Similarly, the **pattern of decreasing the size of the filter** (kernel) with depth was used, starting from the smaller size of 11x11 and decreasing to 5x5, and then to 3x3 in the deeper layers. **The use of small filters such as 5x5 and 3x3 is now the norm**.\n","\n","The pattern of a convolutional layer followed by a pooling layer was used at the start and end of the feature detection part of the model. Interestingly, **a pattern of a convolutional layer followed immediately by a second convolutional** layer was used. **This pattern too has become a modern standard**. \n","\n","To reduce overfitting, the authors used **two regularization techniques**. First, they applied dropout with a 50% dropout rate during training to the outputs of layers F9 and F10. Second, they performed **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n","\n","**Data augmentation** artificially increases the size of the training set by generating many realistic variants of each training instance. **This reduces overfitting**, making this a regularization technique. The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).\n","\n","AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called **Local Response Normalization (LRN)**: the most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization.\n","\n","\n","\n","We can summarize the key aspects of the architecture relevant in modern models as follows:\n","\n","- Use of the ReLU activation function after convolutional layers and softmax for the output layer.\n","- Use of Max Pooling instead of Average Pooling.\n","- Use of Dropout regularization between the fully connected layers.\n","- The pattern of a convolutional layer fed directly to another convolutional layer.\n","- Use of Data Augmentation.\n","\n","\n","> A variant of AlexNet called [ZF Net](https://arxiv.org/abs/1311.2901) was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters (number of feature maps, kernel size, stride, etc.)."]},{"cell_type":"markdown","metadata":{"id":"ApYpc47MFOYi"},"source":["## Step 01: Setup"]},{"cell_type":"markdown","metadata":{"id":"ULBka-lQFJW9"},"source":["Start out by installing the experiment tracking library and setting up your free W&B account:\n","\n","\n","*   **pip install wandb** – Install the W&B library\n","*   **import wandb** – Import the wandb library\n","*   **wandb login** – Login to your W&B account so you can log all your metrics in one place"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWnFIWPuFXej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666461148849,"user_tz":180,"elapsed":8399,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"ffb706fa-395b-4c0d-b6a7-92aaa8b15e2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.9 MB 4.9 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 67.7 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 65.3 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 70.1 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 65.8 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 72.5 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 71.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 61.0 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 61.4 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 74.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 75.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 74.3 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 47.4 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 75.6 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 65.9 MB/s \n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install wandb -qU"]},{"cell_type":"code","source":["# a Python package for tracking the carbon emissions produced by various\n","# kinds of computer programs, from straightforward algorithms to deep neural networks.\n","!pip install codecarbon"],"metadata":{"id":"NBqatao0jyp5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666461159216,"user_tz":180,"elapsed":7328,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"e5f90255-41e3-4ace-c627-f1aeeda10d8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting codecarbon\n","  Downloading codecarbon-2.1.4-py3-none-any.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from codecarbon) (1.3.5)\n","Collecting arrow\n","  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n","\u001b[K     |████████████████████████████████| 66 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from codecarbon) (7.1.2)\n","Collecting py-cpuinfo\n","  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n","\u001b[K     |████████████████████████████████| 99 kB 10.6 MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from codecarbon) (5.4.8)\n","Collecting pynvml\n","  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 5.0 MB/s \n","\u001b[?25hCollecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from codecarbon) (2.23.0)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from arrow->codecarbon) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from arrow->codecarbon) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.15.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->codecarbon) (1.21.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->codecarbon) (2022.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->codecarbon) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->codecarbon) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->codecarbon) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->codecarbon) (3.0.4)\n","Building wheels for collected packages: py-cpuinfo\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=a9f1700fc613e07e9194b80a2774691f932c0a361155d0aa0e77822632dc57c8\n","  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n","Successfully built py-cpuinfo\n","Installing collected packages: pynvml, py-cpuinfo, fuzzywuzzy, arrow, codecarbon\n","Successfully installed arrow-1.2.3 codecarbon-2.1.4 fuzzywuzzy-0.18.0 py-cpuinfo-8.0.0 pynvml-11.4.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"wcrOk6pURp50"},"source":["### Import Packages"]},{"cell_type":"code","source":["# import the necessary packages\n","import logging\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras import backend as K\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from codecarbon import EmissionsTracker\n","from tensorflow.keras.callbacks import Callback\n","from wandb.keras import WandbCallback\n","import wandb"],"metadata":{"id":"8mdgS_E51DwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"id":"7ee2tdx7Cnfs","colab":{"base_uri":"https://localhost:8080/","height":161},"executionInfo":{"status":"ok","timestamp":1666461324389,"user_tz":180,"elapsed":31405,"user":{"displayName":"Ivanovitch Silva","userId":"16824293402572065120"}},"outputId":"5feb7d38-3be3-4b90-b835-88afbb1cdf72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "]},{"name":"stdout","output_type":"stream","text":["··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# configure logging\n","# reference for a logging obj\n","logger = logging.getLogger()\n","\n","# set level of logging\n","logger.setLevel(logging.INFO)\n","\n","# create handlers\n","c_handler = logging.StreamHandler()\n","c_format = logging.Formatter(fmt=\"%(asctime)s %(message)s\",datefmt='%d-%m-%Y %H:%M:%S')\n","c_handler.setFormatter(c_format)\n","\n","# add handler to the logger\n","logger.handlers[0] = c_handler"],"metadata":{"id":"Xq0zG_132yC2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s3MfpRAUMGq6"},"source":["## Implementing the AlexNet model"]},{"cell_type":"markdown","source":["<center><img width=\"400\" src=\"https://drive.google.com/uc?export=view&id=193aOD83q_m_apxqjv1kFSjGRYFV7HfL2\"></center><center>AlexNet Architecture.</center>\n"],"metadata":{"id":"R_qekwmo1w5m"}},{"cell_type":"code","source":["class AlexNet:\n","  ''' \n","  # create AlexNet model\n","  #\n","  # it is composed of the 9 layers \n","  # such as:\n","  #      - 2 blocks CONV => RELU => POOL\n","  #      - 3 blocks CONV => RELU\n","  #      - 1 flatten layer\n","  #      - 2 fully connected layers\n","  #      - 1 output layer with 1000 outputs\n","  #      - input shape = (227,227,3)\n","  '''\n","  @staticmethod\n","  def build(width, height, depth, classes):\n","    # initialize the model\n","    model = Sequential()\n","    inputShape = (height, width, depth)\n","    \n","    # if we are using \"channels first\", update the input shape\n","    if K.image_data_format() == \"channels_first\":\n","      inputShape = (depth, height, width)\n","   \n","    # Block #1: first CONV => RELU => POOL layer set\n","    model.add(Conv2D(96, (11, 11), strides=(4, 4),\n","                    input_shape=inputShape, padding=\"valid\",\n","                    kernel_regularizer=l2(0.0002),activation='relu'))\n","\n","    # Batch Normalization does not exist in 2012, here is a modification of original proposal\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    # In the original paper dropout was used only in FC layers\n","    model.add(Dropout(0.25))\n","\n","    # Block #2: second CONV => RELU => POOL layer set\n","    model.add(Conv2D(256, (5, 5), padding=\"same\",\n","                    kernel_regularizer=l2(0.0002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    # Block #3: CONV => RELU => CONV => RELU => CONV => RELU\n","    model.add(Conv2D(384, (3, 3), padding=\"same\",\n","                    kernel_regularizer=l2(0.0002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(384, (3, 3), padding=\"same\",\n","                    kernel_regularizer=l2(0.002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Conv2D(256, (3, 3), padding=\"same\",\n","                    kernel_regularizer=l2(0.002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n","    model.add(Dropout(0.25))\n","\n","    # Block #4: first set of FC => RELU layers\n","    model.add(Flatten())\n","    model.add(Dense(4096, kernel_regularizer=l2(0.0002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    # Block #5: second set of FC => RELU layers\n","    model.add(Dense(4096, kernel_regularizer=l2(0.0002),activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    # softmax classifier\n","    model.add(Dense(classes, kernel_regularizer=l2(0.0002)))\n","    model.add(Activation(\"softmax\"))\n","        \n","    # return the constructed network architecture\n","    return model"],"metadata":{"id":"t7VrY57Q1mvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a model object\n","model = AlexNet.build(227,227,3,1000)\n","\n","# summarize layers\n","model.summary()"],"metadata":{"id":"XJ1POK1t3hOq"},"execution_count":null,"outputs":[]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXDbwvnXH7u_"
      },
      "source": [
        "# 1.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teKSGqKplfXo"
      },
      "source": [
        "There are discrete architectural elements from milestone models that you can use to design your convolutional neural networks. Specifically, models that have achieved state-of-the-art results for tasks like image classification use discrete architecture elements repeated multiple times, such as the VGG block in the [VGG](https://arxiv.org/abs/1409.1556) models, the inception module in the [GoogLeNet](https://arxiv.org/abs/1409.4842), and the residual module in the [ResNet](https://arxiv.org/abs/1512.03385). Once you can implement parameterized versions of these architectural elements, you can use them to design your models for computer\n",
        "vision and other applications. This notebook will discover how to implement the critical architecture elements from milestone convolutional neural network models from scratch. After completing this lesson, you will know:\n",
        "\n",
        "- How to implement a VGG module used in the VGG-16 and VGG-19 convolutional neural network models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itx0EimHmH2n"
      },
      "source": [
        "# 2.0 VGGnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Uv0SNunzV9"
      },
      "source": [
        "In our previous lesson, we discussed LeNet and AlexNet, two seminal Convolutional Neural Networks in the deep learning and computer vision literature. Simonyan and Zisserman first introduced VGGnet, (sometimes referred to as simply VGG) in their 2015 paper, [Very Deep Learning Convolutional Neural Networks for Large-Scale Image Recognition]((https://arxiv.org/abs/1409.1556)). \n",
        "\n",
        "> The primary contribution of their work was demonstrating that architecture with tiny (3x3) filters can be trained to increasingly higher depths (16-19 layers) and obtain state-of-the-art classification on the challenging ImageNet classification challenge.\n",
        "\n",
        "This network is **characterized by its simplicity**, using only 3×3 convolutional layers stacked on top of each other in increasing depth. **Reducing the spatial dimensions of volumes is accomplished through the usage of max pooling**. Two fully connected layers, each with 4,096 nodes (and dropout in between), are followed by a softmax classifier.\n",
        "\n",
        "VGG is often used today for <font color=\"red\">transfer learning</font> (we will describe this technique later in this course) **as the network demonstrates an above-average ability to generalize to datasets it was not trained on** (compared to other network types such as GoogLeNet and ResNet). More times than not, if you are reading a publication or lab journal that applies transfer learning, it likely uses VGG as the base model.\n",
        "\n",
        "Unfortunately, training VGG from scratch is a pain, to say the least. The network is brutally slow to train, and the network architecture weights themselves are quite large (over 500MB). Due to the depth of the network along with the fully-connected layers, the backpropagation phase is excruciatingly slow.\n",
        "\n",
        "> References from practitioners such as [Adrian Rosebrock](https://www.pyimagesearch.com/), training VGG on eight GPUs took $\\approx$ 10 days – with any less than four GPUs, training VGG from scratch will likely take prohibitively long (unless you can be very patient).\n",
        "\n",
        "That said, it’s important as a deep learning practitioner to understand the history of deep learning, especially the concept of <font color=\"red\">pre-training</font> and how **we later learned to avoid this expensive operation by optimizing our initialization weight functions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SMwntsHn54F"
      },
      "source": [
        "## 2.1 Implementing VGGNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRzfkf_Hz3wC"
      },
      "source": [
        "When implementing VGG, Simonyan and Zisserman tried variants of VGG that increased in depth. The figure below was extracted from their publication is and highlights their experiments. In particular, we are most interested in configurations A, B, D, and E. Looking at these architectures, you will notice two patterns:\n",
        "\n",
        "1. The first is that the **network uses only 3×3 filters**. \n",
        "2. The second is as the depth of the network increases, the number of filters learned increases as well – to be exact, **the number of filters doubles each time max pooling is applied** to reduce volume size. \n",
        "\n",
        "> The notion of doubling the number of filters each time you decrease spatial dimensions is of historical importance in the deep learning literature and even a pattern you will see today.\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=19cSs4edO6rQ4ZodRzgU54Inv8P3m566K\"/>\n",
        "\n",
        "We perform this doubling of filters to ensure no single layer block is more biased than the others. Layers earlier in the network architecture have fewer filters, but their spatial volumes are also much larger, implying there is **more (spatial) data** to learn from.\n",
        "\n",
        "However, we know that applying a max-pooling operation will reduce our spatial input volumes. \n",
        "\n",
        "> If we reduce the spatial volumes without increasing the number of filters, our layers become unbalanced and potentially biased, implying that layers earlier in the network may influence our output classification more than layers deeper. \n",
        "\n",
        "To combat this imbalance, we keep in mind the ratio of volume size to the number of filters. If we reduce the input volume size by 50-75%, we double the number of filters in the next set of CONV layers to maintain the balance.\n",
        "\n",
        "The issue with training such deep architectures is that Simonyan and Zisserman found training VGG16 and VGG19 to be extremely challenging due to their depth. **If these architectures were randomly initialized and trained from scratch, they would often struggle to learn and gain any initial traction – the networks were too deep for basic random initialization**. Therefore, to train deeper variants of VGG, Simonyan and Zisserman came up with a clever concept called <font color=\"red\">pre-training</font>.\n",
        "\n",
        "> Pre-training is the practice of training smaller versions of your network architecture with fewer weight layers first and then using these converged network weights as the initializations for larger,\n",
        "deeper networks.\n",
        "\n",
        "In the case of VGG, the authors first trained configuration A, VGG11. VGG11 was able to converge to the level of reasonably low loss but not state-of-the-art accuracy worthy.\n",
        "\n",
        "The weights from VGG11 were then used as initializations to configuration B, VGG13. The conv3-64 and conv3-128 layers (highlighted in bold in above figure) in VGG13 were randomly initialized while the remainder of the layers were copied over from the pre-trained VGG11 network. Using the initializations, Simonyan and Zisserman were able to train VGG13 successfully–but still not obtain state-of-the-art accuracy.\n",
        "\n",
        "This pre-training pattern continued to configuration D, which we commonly know as VGG16. This time three new layers were randomly initialized while the other layers were copied over from VGG13. The network was then trained using these **warmed pre-trained up** layers, thereby allowing the randomly initialized layers to converge and learn discriminating patterns. Ultimately, VGG16 was able to perform very well on the ImageNet classification challenge.\n",
        "\n",
        "> As a final experiment, Simonyan and Zisserman once again applied pre-training to configuration E, VGG19. This very deep architecture copied the weights from the pre-trained VGG16 architecture and then added another additional three convolutional layers. After training, it was found that VGG19 obtained the highest classification accuracy from their experiments; however, the size of the model (574MB) and the amount of time it took to train and evaluate the network, all for meager gains, made it less appealing to deep learning practitioners.\n",
        "\n",
        "**If pre-training sounds like a painful, tedious process, that is because it is**. Training smaller variations of your network architecture and then using the converged weights as initializations to your deeper versions of the network is a clever trick; however, it requires training and tuning the\n",
        "hyperparameters to N separate networks, where N is your final network architecture along with the number of previous (smaller) networks required to obtain the end model. Performing this process is\n",
        "extremely time-consuming, especially for deeper networks with many fully-connected layers such as VGG.\n",
        "\n",
        "<font color=\"red\">The good news is that we no longer perform pre-training</font> when training very deep Convolutional Neural Networks – instead, we rely on a good initialization function. Instead of pure random weight initializations we now use [Xavier/Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) or MSRA (also known as He et al. initialization).\n",
        "Through the work of both Mishkin and Mtas in their 2015 paper, [All you need is a good init](https://arxiv.org/abs/1511.06422) and He et al. in [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852), we found that we can skip the pre-training phase entirely and jump directly to the deeper variations of the network architectures.\n",
        "\n",
        "> After these papers were published, Simonyan and Zisserman re-evaluated their experiments and found that these **smarter initialization** schemes and activation functions could replicate their previous performance without the usage of tedious pre-training.\n",
        "\n",
        "Additionally, **we recommend using batch normalization after the activation functions in the network**. \n",
        "\n",
        "> Apply batch normalization was not discussed in the original Simonyan and Zisserman paper, but as other lessons have discussed, batch normalization can stabilize your training and reduce the total number of epochs required to obtain a reasonably performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xBJgBMTDl-Q"
      },
      "source": [
        "## 2.2 The VGG Family of Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9mrWjqWUjhz"
      },
      "source": [
        "Two key components can characterize the VGG family of Convolutional Neural Networks:\n",
        "\n",
        "1. All CONV layers in the network using only 3x3 filters.\n",
        "2. Stacking multiple CONV => RELU layer sets (where the number of consecutive CONV => RELU layers typically increases the deeper we go) before applying a POOL operation.\n",
        "\n",
        "In this section, we will discuss a variant of the VGGNet architecture, which we call “MiniVGGNet,” because the network is substantially more shallow than its big brother."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "model = VGG16(weights=\"imagenet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KBOaCfgfSF1",
        "outputId": "56d3be36-84f3-4cca-c182-249625c007a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "zo_TqkbofS5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the layers in the network and display them to the console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "  print(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oqNDqJKjdK3",
        "outputId": "8d9fb01d-fec8-4aa8-d462-04800ce7bb7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] 0\tInputLayer\n",
            "[INFO] 1\tConv2D\n",
            "[INFO] 2\tConv2D\n",
            "[INFO] 3\tMaxPooling2D\n",
            "[INFO] 4\tConv2D\n",
            "[INFO] 5\tConv2D\n",
            "[INFO] 6\tMaxPooling2D\n",
            "[INFO] 7\tConv2D\n",
            "[INFO] 8\tConv2D\n",
            "[INFO] 9\tConv2D\n",
            "[INFO] 10\tMaxPooling2D\n",
            "[INFO] 11\tConv2D\n",
            "[INFO] 12\tConv2D\n",
            "[INFO] 13\tConv2D\n",
            "[INFO] 14\tMaxPooling2D\n",
            "[INFO] 15\tConv2D\n",
            "[INFO] 16\tConv2D\n",
            "[INFO] 17\tConv2D\n",
            "[INFO] 18\tMaxPooling2D\n",
            "[INFO] 19\tFlatten\n",
            "[INFO] 20\tDense\n",
            "[INFO] 21\tDense\n",
            "[INFO] 22\tDense\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7leU0DAXFJV"
      },
      "source": [
        "# 3.0 GoogLeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3LfKTWUXIjW"
      },
      "source": [
        "This section will study the **GoogLeNet architecture**, introduced by Szegedy et al. in their 2014 paper, [Going Deeper With Convolutions](https://arxiv.org/pdf/1409.4842.pdf). This paper is essential for two reasons. First, the model architecture is tiny compared to AlexNet and VGGNet ( 28MB for the weights themselves). The authors can obtain such a dramatic drop in network architecture size (while still increasing the depth of the overall network) by removing fully connected layers and instead of using global average pooling. Most of the weights in a CNN can be found in the dense FC layers – if these layers can be removed, the memory savings are massive.\n",
        "\n",
        "Secondly, the Szegedy et al. paper makes usage of a network in-network or micro-architecture when constructing the overall macro-architecture. Up to this point, we have seen only sequential neural networks where the output of one network feeds directly into the next. We are now going to see micro-architectures, small building blocks used inside the rest of the architecture, where the output from one layer can split into various paths and be rejoined later.\n",
        "\n",
        "Specifically, Szegedy et al. contributed the Inception module to the deep learning community, a building block that fits into a Convolutional Neural Network, enabling it to learn CONV layers with **multiple filter sizes**, turning the module into a multi-level feature extractor.\n",
        "\n",
        "Micro-architectures such as Inception have inspired other significant variants, including the Residual module in [ResNet](https://arxiv.org/abs/1512.03385) and the Fire module in [SqueezeNet](https://arxiv.org/abs/1602.07360). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KUD4f1bhL7"
      },
      "source": [
        "## 3.1 The Inception Module (and its Variants)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYKoL0TxcSxi"
      },
      "source": [
        "Modern state-of-the-art Convolutional Neural Networks utilize **micro-architectures**, also called **network-in-network modules**, initially proposed by [Lin et al](https://arxiv.org/abs/1312.4400). I prefer the term micro-architecture better describes these modules as building blocks in the context of the overall macro-architecture (i.e., what you build and train).\n",
        "\n",
        "Micro-architectures are tiny building blocks designed by deep learning practitioners to enable networks to learn (1) faster and (2) more efficiently, all while increasing network depth. \n",
        "> These micro-architecture building blocks are stacked, along with conventional layer types such as CONV, POOL, etc., to form the overall macro-architecture.\n",
        "\n",
        "In 2014, Szegedy et al. introduced the Inception module. The general idea behind the Inception module is two-fold:\n",
        "\n",
        "1. **It can be hard to decide the size of the filter you need to learn at given CONV layers**. Should they be 5x5 filters? What about 3x3 filters? Should we learn local features using 1x1 filters? Instead, why not learn them all and let the model decide? Inside the Inception module, we learn all three 5x5, 3x3, and 1x1 filters (computing them in parallel), concatenating the resulting feature maps along the channel dimension. The next layer in\n",
        "the GoogLeNet architecture (which could be another Inception module) receives these concatenated, mixed filters and performs the same process. Taken as a whole, this process enables GoogLeNet to learn both local features via smaller convolutions and abstracted\n",
        "features with larger convolutions – we do not have to sacrifice our level of abstraction at the expense of smaller features.\n",
        "2. By learning multiple filter sizes, we can turn the module into a multi-level feature extractor. The 5x5 filters have a larger receptive size and can learn more abstract features. The 1x1 filters are, by definition, local. The 3x3 filters sit as a balance in between."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ap9n_4df8NB"
      },
      "source": [
        "### 3.1.1 Inception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIm6Tf14gDi_"
      },
      "source": [
        "Now that we’ve discussed the motivation behind the Inception module, let’s look at the actual\n",
        "module itself in Figure below (the original Inception module used in GoogLeNet).\n",
        "\n",
        "<img width=\"450\" src=\"https://drive.google.com/uc?export=view&id=1ja1dLUSBtZSxBwsbimHUbho0cEvno0Cb\"/>\n",
        "\n",
        "Specifically, take note of how the Inception module branches into four distinct paths from the input layer. The first branch in the Inception module learns a series of 1x1 local features from the input.\n",
        "\n",
        "The second batch first applies 1x1 convolutions, not only as a form of learning local features but instead as dimensionality reduction. Larger convolutions (i.e., 3x3 and 5x5) by definition take more computation to perform. Therefore, if we can reduce the dimensionality of the inputs\n",
        "to these larger filters by applying 1x1 convolutions, we can reduce the computation required by our network. Therefore, the number of filters learned in the 1x1 CONV in the second branch will always be smaller than the number of 3x3 filters learned directly afterward.\n",
        "\n",
        "The third branch applies the same logic as the second branch, only this time to learn 5x5 filters. We once again reduce dimensionality via 1x1 convolutions, then feed the output into the 5x5 filters.\n",
        "\n",
        "The fourth and final branch of the Inception module performs 3x3 max pooling with a stride of 1x1 – this branch is commonly referred to as the pool projection branch. Historically, models that perform pooling have demonstrated an ability to obtain higher accuracy, although we now\n",
        "know through the work of Springenberg et al. in their 2014 paper, [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806) that this is not necessarily true and that POOL layers can be replaced with CONV layers for reducing volume size.\n",
        "\n",
        "In the case of Szegedy et al., this POOL layer was added simply because it was thought that they were needed for CNNs to perform reasonably. The output of the POOL is then fed into another series of 1x1 convolutions to learn local features.\n",
        "\n",
        "Finally, all four-interception modules converge where they are concatenated together along the channel dimension. Special care is taken during the implementation (via zero paddings) to ensure the output of each branch has the same volume size, thereby allowing the outputs to be concatenated. The output of the Inception module is then fed into the next layer in the network. In practice, we often stack multiple Inception modules on top of each other before performing a pooling operation to reduce volume size."
      ]
    }
  ]
}
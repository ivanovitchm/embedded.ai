{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-4bHFI0Fz4"
      },
      "source": [
        "# 1 - How to Use Pre-Trained Models and Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHGxu8MV7yP3"
      },
      "source": [
        "**Deep convolutional neural network models may take days or even weeks** to train on very large datasets. A way to short-cut this process is to re-use the model weights from pre-trained models that were developed for standard computer vision benchmark datasets, such as the ImageNet image recognition tasks. Top performing models can be downloaded and used directly, or integrated into a new model for your own computer vision problems. In this lesson, you\n",
        "will discover how to use transfer learning when developing convolutional neural networks for computer vision applications. After reading this notebook, you will know:\n",
        "\n",
        "- Transfer learning involves using models trained on one problem as a starting point on a related problem.\n",
        "- Transfer learning is flexible, allowing the use of pre-trained models directly, as feature extraction preprocessing, and integrated into entirely new models.\n",
        "- Keras provides convenient access to many top performing models on the ImageNet image recognition tasks such as VGG, Inception, and ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNajNQEn8REt"
      },
      "source": [
        "## 1.1 What is Transfer Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xROgzHd-qUm"
      },
      "source": [
        "**Transfer learning** generally refers to a process where a **model trained on one problem is used in some way on a second**, related problem. In deep learning, transfer learning is a technique whereby a neural network model is first trained on a problem similar to the problem that is being solved. One or more layers from the trained model are then used in a new model trained on the problem of interest.\n",
        "\n",
        "> This is typically understood in a supervised learning context, where the input is the same but the target may be of a different nature. For example, we may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.\n",
        "\n",
        "Transfer learning has the **benefit of decreasing the training time** for a neural network model and can result in lower generalization error. The weights in re-used layers may be used as the starting point for the training process and adapted in response to the new problem. This usage treats transfer learning as a type of weight initialization scheme. This may be useful when the first related problem has a lot more labeled data than the problem of interest and the similarity in the structure of the problem may be useful in both contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE5BvQON-0GR"
      },
      "source": [
        "## 1.2 How to Use Pre-Trained Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNwbOFaPANrY"
      },
      "source": [
        "A range of high-performing models have been developed for image classification and demonstrated on the annual ImageNet Large Scale Visual Recognition Challenge, or ILSVRC. This challenge, often referred to simply as ImageNet, given the source of the image used in the competition, has resulted in a number of innovations in the architecture and training of convolutional neural networks. In addition, many of the models used in the competitions have been released under\n",
        "a permissive license. These models can be used as the basis for transfer learning in computer vision applications. This is desirable for a number of reasons, not least:\n",
        "\n",
        "- **Useful Learned Features**: The models have learned how to detect generic features from photographs, given that they were trained on more than 1,000,000 images for 1,000\n",
        "categories.\n",
        "- **State-of-the-Art Performance**: The models achieved state-of-the-art performance and remain effective on the specific image recognition task for which they were developed.\n",
        "- **Easily Accessible**: The model weights are provided as free downloadable files and many libraries provide convenient APIs to download and use the models directly.\n",
        "\n",
        "The model weights can be downloaded and used in the same model architecture using a range of different deep learning libraries, including Keras.\n",
        "\n",
        "\n",
        "**The use of a pre-trained model is limited only by your creativity**. For example, a model may be downloaded and used as-is, such as embedded into an application and used to classify new photographs. Alternately, models may be downloaded and used as feature extraction models. Here, the output of the model from a layer prior to the output layer of the model is used as input to a new classifier model. Recall that convolutional layers closer to the input layer of the model learn low-level features such as lines, that layers in the middle of the layer learn complex abstract features that combine the lower level features extracted from the input, and layers closer to the output interpret the extracted features in the context of a classification task.\n",
        "\n",
        "Armed with this understanding, a level of detail for feature extraction from an existing pre-trained model can be chosen. For example, if a new task is quite different from classifying objects in photographs (e.g. different to ImageNet), then perhaps the output of the pre-trained\n",
        "model after the first few layers would be appropriate. If a new task is quite similar to the task of classifying objects in photographs, then perhaps the output from layers much deeper in the model can be used, or even the output of the fully connected layer prior to the output layer can be used.\n",
        "\n",
        "The pre-trained model can be used as a separate feature extraction program, in which case input can be pre-processed by the model or portion of the model to a given an output (e.g. vector of numbers) for each input image, that can then used as input when training a new model. Alternately, the pre-trained model or desired portion of the model can be integrated directly into a new neural network model. In this usage, the weights of the pre-trained can be frozen so that they are not updated as the new model is trained. Alternately, the weights may be updated during the training of the new model, perhaps with a lower learning rate, allowing the pre-trained model to act like a weight initialization scheme when training the new model. We can summarize some of these usage patterns as follows:\n",
        "\n",
        "- **Classifier**: The pre-trained model is used directly to classify new images.\n",
        "- **Standalone Feature Extractor**: The pre-trained model, or some portion of the model, is used to pre-process images and extract relevant features.\n",
        "- **Integrated Feature Extractor**: The pre-trained model, or some portion of the model, is integrated into a new model, but layers of the pre-trained model are frozen during\n",
        "training.\n",
        "- **Weight Initialization**: The pre-trained model, or some portion of the model, is integrated into a new model, and the layers of the pre-trained model are trained in concert with the new model.\n",
        "\n",
        "Each approach can be effective and save significant time in developing and training a deep convolutional neural network model. It may not be clear as to which usage of the pre-\n",
        "trained model may yield the best results on your new computer vision task, therefore **some experimentation may be required**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLv0dSxrnwwK"
      },
      "source": [
        "# 2 - Extracting features with a pre-trained CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAGBKpo2n-Zq"
      },
      "source": [
        "Up until this point, we have treated Convolutional Neural Networks as end-to-end image classifiers:\n",
        "\n",
        "1. We input an image to the network.\n",
        "2. The image forward propagates through the network.\n",
        "3. We obtain the final classification probabilities from the end of the network.\n",
        "\n",
        "However, **there is no “rule” that says we must allow the image to forward propagate through\n",
        "the entire network**. \n",
        "\n",
        "> Instead, we can stop the propagation at an arbitrary layer, such as an activation\n",
        "or pooling layer, extract the values from the network at this time, and then use them as feature\n",
        "vectors. \n",
        "\n",
        "For example, let’s consider the VGG16 network architecture by [Simonyan and Zisserman](https://arxiv.org/abs/1409.1556) (Figure below, left).\n",
        "\n",
        "<center><img width=\"500\" src=\"https://drive.google.com/uc?export=view&id=1CNy_EpEVeVAn7LJbPyeZm7xKW1rLnwdz\"></center><center><b>Left</b>: The original VGG16 network architecture that outputs probabilities for each of the 1,000 ImageNet class labels. <b>Right</b>: Removing the FC layers from VGG16 and instead returning the output of the final POOL layer. This output will serve as our extracted features.</center>\n",
        "\n",
        "Along with the layers in the network, we have also included the input and output shapes of the\n",
        "volumes for each layer. When treating networks as a feature extractor, we essentially “chop off” the network at an arbitrary point (normally prior to the fully-connected layers, but it really depends on your particular dataset).\n",
        "\n",
        "Now the last layer in our network is a max pooling layer (Figure above, right) which will have the output shape of 7 x 7 x 512 implying there are 512 filters each of size 7 x 7. If we were to forward propagate an image through this network with its FC head removed, we would be left with 512, 7x7 activations that have either activated or not based on the image contents. Therefore, we can actually take these 7x7x512 = 25,088 values and treat them as a feature vector that **quantifies the contents of an image**.\n",
        "\n",
        "If we repeat this process for an entire dataset of images (including datasets that VGG16 was\n",
        "not trained on), we’ll be left with a design matrix of N images, each with 25,088 columns used to\n",
        "quantify their contents (i.e., feature vectors). Given our feature vectors, we can train an off-the-shelf machine learning model such a **Linear SVM, Logistic Regression classifier, or Random Forest** on top of these features to obtain a classifier that recognizes new classes of images.\n",
        "\n",
        "Keep in mind that the CNN itself is not capable of recognizing these new classes – instead,\n",
        "we are using the CNN as an intermediary feature extractor. The downstream machine learning\n",
        "classifier will take care of learning the underlying patterns of the features extracted from the CNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vq795HpMbO9"
      },
      "source": [
        "## 2.1 Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llvMI_SLLuUZ"
      },
      "outputs": [],
      "source": [
        "# install wandb\n",
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0PopooAL-0r"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from imutils import paths\n",
        "import logging\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "import wandb\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OIDxoimMGaE"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkYBIKLPMIxK"
      },
      "outputs": [],
      "source": [
        "# configure logging\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# set level of logging\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# create handlers\n",
        "c_handler = logging.StreamHandler()\n",
        "c_format = logging.Formatter(fmt=\"%(asctime)s %(message)s\",datefmt='%d-%m-%Y %H:%M:%S')\n",
        "c_handler.setFormatter(c_format)\n",
        "\n",
        "# add handler to the logger\n",
        "logger.handlers[0] = c_handler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMMs-aSgMMvP"
      },
      "source": [
        "## 2.2 Fetch and Data Segregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3OyUC3IMgGS"
      },
      "outputs": [],
      "source": [
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "  \"project_name\": \"first_image_classifier\",\n",
        "  \"artifact_name\": \"animals_raw_data:latest\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mdgxtJpMiT2"
      },
      "outputs": [],
      "source": [
        "# open the W&B project created in the Fetch step\n",
        "run = wandb.init(entity=\"ivanovitch-silva\",project=args[\"project_name\"], job_type=\"preprocessing\")\n",
        "\n",
        "# download the raw data from W&B\n",
        "raw_data = run.use_artifact(args[\"artifact_name\"])\n",
        "data_dir = raw_data.download()\n",
        "logger.info(\"Path: {}\".format(data_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcWhQplNMk4A"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeqcqXAxPF6q"
      },
      "outputs": [],
      "source": [
        "data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzzevQ01PIFa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sY5ingdlIUz"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "raw_wb = tf.keras.utils.image_dataset_from_directory(data_dir,\n",
        "                                                     shuffle=True,\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     image_size=IMG_SIZE,\n",
        "                                                     label_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(raw_wb)"
      ],
      "metadata": {
        "id": "uXzKbKgHxGVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_pIkorUmA1W"
      },
      "outputs": [],
      "source": [
        "# 80% train, 20% validation and test\n",
        "all_batches = tf.data.experimental.cardinality(raw_wb)\n",
        "valtest_dataset = raw_wb.take(all_batches // 5)\n",
        "train_dataset = raw_wb.skip(all_batches // 5)\n",
        "\n",
        "valtest_batches = tf.data.experimental.cardinality(valtest_dataset)\n",
        "test_dataset = valtest_dataset.take(valtest_batches // 5)\n",
        "validation_dataset = valtest_dataset.skip(valtest_batches // 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBe8Oo7bmr7Y"
      },
      "outputs": [],
      "source": [
        "print('Number of raw batches: %d' % tf.data.experimental.cardinality(raw_wb))\n",
        "print('Number of train batches: %d' % tf.data.experimental.cardinality(train_dataset))\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do1Ie9JNqoOW"
      },
      "outputs": [],
      "source": [
        "class_names = raw_wb.class_names\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[tf.argmax(labels[i])])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB6HqPE434Ma"
      },
      "source": [
        "## 2.3 Configure the dataset for performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZoxwItM4Bim"
      },
      "source": [
        "Use buffered prefetching to load images from disk without having I/O become blocking. To learn more about this method see the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFVOsTw738q-"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9LFctxi4Jw3"
      },
      "source": [
        "## 2.4 Use data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Zmpr2T4QUZ"
      },
      "source": [
        "When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. You can learn more about data augmentation in this [tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wU_Jn5M4Mpg"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HE9KgqL4WX5"
      },
      "outputs": [],
      "source": [
        "for image, _ in train_dataset.take(1):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  first_image = image[0]\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "    plt.imshow(augmented_image[0] / 255)\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhANSupp5aJX"
      },
      "source": [
        "## 2.5 Create the base model from the pre-trained convnets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dl41CYQ5o-0"
      },
      "source": [
        "You will create the base model from the [VGG16 model](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16). This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help us classify cats and dogs from our specific dataset.\n",
        "\n",
        "First, you need to pick which layer of VGG16 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
        "\n",
        "First, instantiate a VGG16 model pre-loaded with weights trained on ImageNet. By specifying the **include_top=False** argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhpwZblR6Cxl"
      },
      "outputs": [],
      "source": [
        "# Create the base model from the pre-trained model VGG16\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4-_PhTt6R78"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTfuXpHi6lhP"
      },
      "source": [
        "This feature extractor converts each 224x224x3 image into a 7x7x512 block of features. Let's see what it does to an example batch of images:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1oLaLXF6s2d"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(train_dataset))\n",
        "feature_batch = model(image_batch)\n",
        "print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6nhFW6l9TQb"
      },
      "source": [
        "## 2.6 Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM4dkPIp9W4e"
      },
      "source": [
        "In this step, you will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, you add a classifier on top of it and train the top-level classifier.\n",
        "\n",
        "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. VGG16 has many layers, so setting the entire model's trainable flag to False will freeze all of them. This is crucial only in case we are using [tf.keras.layers.BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) layers in the model.\n",
        "\n",
        "> Many models contain [tf.keras.layers.BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) layers. This layer is a special case and precautions should be taken in the context of fine-tuning, as shown later in this notebook.\n",
        "\n",
        "When you set ``layer.trainable = False``, the **BatchNormalization** layer will run in inference mode, and will not update its mean and variance statistics.\n",
        "\n",
        "When you unfreeze a model that contains **BatchNormalization** layers in order to do fine-tuning, you should keep the **BatchNormalization** layers in inference mode by passing training = False when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model has learned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kG9qHG69gXJ"
      },
      "outputs": [],
      "source": [
        "model.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D-nJci98Hdi"
      },
      "source": [
        "## 2.7 Add a classification head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONZ8uTQn8K5_"
      },
      "source": [
        "To generate predictions from the block of features, average over the spatial 7x7 spatial locations, using a [tf.keras.layers.GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layer to convert the features to a single 512-element vector per image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RpEGwY18VKg"
      },
      "outputs": [],
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCpRD0i_9zF0"
      },
      "source": [
        "Apply a [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer to convert these features into a single prediction per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6xRxnD9965z"
      },
      "outputs": [],
      "source": [
        "prediction_layer = tf.keras.layers.Dense(3,\"softmax\")\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zy1s3eL_Nra"
      },
      "source": [
        "Build a model by chaining together the data augmentation, [tf.keras.applications.vgg16.preprocess_input](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input), model and feature extractor layers using the [Keras Functional API](https://www.tensorflow.org/guide/keras/functional). As previously mentioned, use ``training=False`` is only redundant because our models not contains a BatchNormalization layer. However, use it here is important for the sake of understanding. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxBWO0hgAYIZ"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.vgg16.preprocess_input(x)\n",
        "x = model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUIOZDR1A7vE"
      },
      "source": [
        "## 2.8 Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zuzsRiSBDZJ"
      },
      "source": [
        "Compile the model before training it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKgDMSq4BIYr"
      },
      "outputs": [],
      "source": [
        "# initialize the optimizer and model\n",
        "learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AeI6bLbBoAR"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMLb02ZEB5Xf"
      },
      "source": [
        "The 14.714.688 million parameters in VGG16 are frozen, but there are 1,539 thousand trainable parameters in the Dense layer. These are divided between two [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) objects, the weights and biases.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tuipe9sBCI_N"
      },
      "outputs": [],
      "source": [
        "len(model.trainable_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkkMcu7wESER"
      },
      "source": [
        "## 2.9 Train and Evalute the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbDQ9Ib0ETw8"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset,\n",
        "                    validation_data=validation_dataset,\n",
        "                    batch_size=32,\n",
        "                    epochs=20,\n",
        "                    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b18AFZDqaiKd"
      },
      "outputs": [],
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
        "\n",
        "ax.plot(np.arange(0, 20), history.history[\"loss\"], label=\"train_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 20), history.history[\"val_loss\"], label=\"val_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 20), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "ax.plot(np.arange(0, 20), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "ax.set_title(\"Training Loss and Accuracy\")\n",
        "ax.set_xlabel(\"Epoch #\")\n",
        "ax.set_ylabel(\"Loss/Accuracy\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the dataset is a generator (``tensorflow.python.data.ops.dataset_ops.PrefetchDataset``). Thus, each time you call the dataset object a new (and different) sequence of images will be returned. This is very important to guarantee the same sequence is used in ``model.predict(test_dataset)`` and ``true_labels``. "
      ],
      "metadata": {
        "id": "fTH7yZvSkEck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(test_dataset)"
      ],
      "metadata": {
        "id": "_bF1smELkUbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "y2 = np.concatenate([y for x, y in test_dataset], axis=0)"
      ],
      "metadata": {
        "id": "UzW7A6mmfyKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as previous mentioned, y is different of y2\n",
        "# because test_dataset is a generator\n",
        "assert y == y2"
      ],
      "metadata": {
        "id": "iiy_tB72k2cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A possible solution is to extract a snapshot of ``test_dataset``."
      ],
      "metadata": {
        "id": "jCk-V_HZlIsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = tuple(zip(*test_dataset))\n",
        "images = np.concatenate(images,axis=0)\n",
        "labels = np.concatenate(labels,axis=0)"
      ],
      "metadata": {
        "id": "jEAMds8ojEpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQNRFyzqaxe8"
      },
      "outputs": [],
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(images, batch_size=32)\n",
        "print(classification_report(tf.argmax(labels, axis=1),\n",
        "                            tf.argmax(predictions, axis=1),\n",
        "                            target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6uRo7vVHz9C"
      },
      "outputs": [],
      "source": [
        "fig_confusion_matrix, ax = plt.subplots(1,1,figsize=(7,4))\n",
        "ConfusionMatrixDisplay(confusion_matrix(tf.argmax(predictions, axis=1),\n",
        "                                        tf.argmax(labels, axis=1)),\n",
        "                       display_labels=class_names).plot(values_format=\".0f\",ax=ax)\n",
        "\n",
        "ax.set_xlabel(\"True Label\")\n",
        "ax.set_ylabel(\"Predicted Label\")\n",
        "ax.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy_5eGBhJJi"
      },
      "source": [
        "# 3 - Fine-tuning networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVh5JwSiZrj"
      },
      "source": [
        "In the previous section we learned how to treat a pre-trained **Convolutional Neural Network** as **feature extractor**. \n",
        "\n",
        "> Using this feature extractor, we forward propagated our dataset of images through the network, extracted the activations at a given layer, and saved the values to disk. A shallow machine\n",
        "learning classifier was then trained on top of the CNN features.\n",
        "\n",
        "This CNN feature extractor approach, called **transfer learning**, obtained remarkable accuracy, far higher than any of our previous experiments on the Animals dataset.\n",
        "\n",
        "But there is another type of transfer learning, one that can actually outperform the feature extraction method if you have sufficient data. This method is called **fine-tuning** and **requires us to perform “network surgery”**. \n",
        "\n",
        "1. First, we take a **scalpel and cut off the final set of fully-connected layers** (i.e., the “head” of the network) from a pre-trained /Convolutional Neural Network, such as\n",
        "VGG, ResNet, Inception, so on. \n",
        "2. We then **replace the head** with a new set of fully-connected layers with random initializations. From there all layers below the head are frozen so their weights cannot be\n",
        "updated (i.e., the backward pass in backpropagation does not reach them)\n",
        "3.  We then train the network **using a very small learning rate** so the new set of FC layers can start to learn patterns from the previously learned CONV layers earlier in the network. \n",
        "4. Optionally, we may unfreeze the rest of the network and continue training. Applying fine-tuning allows us to apply pre-trained networks to recognize classes that they were not originally trained on; furthermore, **this method can lead to higher accuracy than feature extraction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwpP_T-fknnS"
      },
      "source": [
        "## 3.1 Transfer Learning and Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deGXnCSBksDi"
      },
      "source": [
        "**Fine-tuning is a type of transfer learning**. We apply fine-tuning to deep learning models that have already been trained on a given dataset. Typically, these networks are state-of-the-art architectures\n",
        "such as VGG, ResNet, and Inception that have been trained on the ImageNet dataset.\n",
        "\n",
        "As we found out in previous section on feature extraction, these networks contain rich, discriminative filters that can be used on datasets and class labels outside the ones they have already been trained on. However, instead of simply applying feature extraction, we are going to perform network surgery and modify the actual architecture so we can re-train parts of the network.\n",
        "\n",
        "\n",
        "If this sounds like something out of a bad horror movie; don’t worry, there won’t be any blood and gore – but we will have some fun and learn a lot with our experiments. To understand how finetuning\n",
        "works, consider Figure below (left) where we have the layers of the VGG16 network. As we know, the final set of layers (i.e., the “head”) are our fully-connected layers along with our softmax classifier. When performing fine-tuning, we actually remove the head from the network, just as in feature extraction (middle). However, unlike feature extraction, when we perform fine-tuning we actually **build a new fully-connected head and place it on top of the original architecture\n",
        "(right)**.\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1qTj4KeosAyDUcffqTQ_BepiINEUXs-cE\"></center><center><b>Left</b>:  The original VGG16 network architecture. <b>Middle</b>: Removing the FC layers from VGG16 and treating the final POOL layer as a feature extractor. <b>Right</b>: Removing the original FC layers and replacing them with a brand new FC head. These new FC layers can then be fine-tuned to the specific dataset (the old FC layers are no longer used).</center>\n",
        "\n",
        "\n",
        "In most cases your new FC head will have fewer parameters than the original one; however, that really depends on your particular dataset. The new FC head is randomly initialized (just like any other layer in a new network) and connected to the body of the original network, and we are ready to train.\n",
        "\n",
        "However, there is a problem – our CONV layers have already learned rich, discriminating filters while our FC layers are brand new and totally random. If we allow the gradient to backpropagate from these random values all the way through the body of our network, we risk destroying these powerful features. To circumvent this, we instead let our FC head “warm up” by (ironically) “freezing” all layers in the body of the network as\n",
        "in Figure below (left).\n",
        "\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=11Zh6mGG3qMISsnCg6JLgL-sH7TnxpUSC\"></center><center><b>Left</b>: When we start the fine-tuning process we freeze all CONV layers in the network and only allow the gradient to backpropagate through the FC layers. Doing this allows our network to “warm up”. <b>Right</b>: After the FC layers have had a chance to warm up we may choose to unfreeze all layers in the network and allow each of them to be fine-tuned as well.</center>\n",
        "\n",
        "\n",
        "Training data is forward propagated through the network as we normally would; however, the backpropagation is stopped after the FC layers, which allows these layers to start to learn patterns from the highly discriminative CONV layers. In some cases, we may never unfreeze the body of the network as our new FC head may obtain sufficient accuracy. \n",
        "\n",
        "However, for some datasets it is often advantageous to allow the original CONV layers to be modified during the fine-tuning process as\n",
        "well (Figure above, right).\n",
        "\n",
        "After the FC head has started to learn patterns in our dataset, pause training, unfreeze the body, and then continue the training, but with a very **small learning rate** – we do not want to deviate our\n",
        "CONV filters dramatically. \n",
        "\n",
        "Training is then allowed to continue until sufficient accuracy is obtained. Fine-tuning is a super powerful method to obtain image classifiers from pre-trained CNNs on custom datasets, even more powerful than feature extraction in most cases. **The downside is that\n",
        "fine-tuning can require a bit more work and your choice in FC head parameters does play a big part\n",
        "in network accuracy** – you can’t rely strictly on regularization techniques here as your network has already been pre-trained and you can’t deviate from the regularization already being performed by\n",
        "the network.\n",
        "\n",
        "Secondly, for small datasets, it can be challenging to get your network to start “learning” from a “cold” FC start, which is why we freeze the body of the network first. Even still, getting past the warm-up stage can be a bit of a challenge and might require you to use optimizers other than SGD. **While fine-tuning does require a bit more effort, if it is done correctly, you’ll nearly always enjoy higher accuracy**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQ6aKJgaalh"
      },
      "source": [
        "## 3.2 Indexes and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p37ZtS0IwXBs"
      },
      "source": [
        "Prior to performing **network surgery**, we need to know the **layer name and index** of every layer in a given deep learning model. We need this information as we’ll be required to **“freeze”** and **“unfreeze”** certain layers in a pre-trained CNN.\n",
        "\n",
        "Without knowing the layer names and indexes ahead of time, we would be “cutting blindly”, an out-of-control surgeon with no game plan. **If we instead take a few minutes to examine the network architecture and implementation, we can better prepare for our surgery.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-1lFYebUiP"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# whether or not to include top of CNN\n",
        "include_top = 0\n",
        "\n",
        "# load the VGG16 network\n",
        "print(\"[INFO] loading network...\")\n",
        "model = VGG16(weights=\"imagenet\", include_top= include_top > 0)\n",
        "print(\"[INFO] showing layers...\")\n",
        "\n",
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e64KpW4KzcBL"
      },
      "source": [
        "Before we can replace the head of a pre-trained CNN, we need something to replace it with – therefore, we need to define our own fully-connected head of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZw23cpCeZ_m"
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# a fully connect network\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, neurons):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(neurons, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bZQdHFwzq7t"
      },
      "source": [
        "Again, this fully-connected head is very simplistic compared to the original head from VGG16 which consists of two sets of 4,096 FC layers. However, for most fine-tuning problems you are not seeking to replicate the original head of the network, but rather simplify it so it is easier to fine-tune– the fewer parameters in the head, the more likely we’ll be to correctly tune the network to a new\n",
        "classification task.\n",
        "\n",
        "\n",
        "In some cases you’ll want to allow the entire body to be trainable; however, **for deeper architectures with many parameters such as VGG, I suggest only unfreezing the top CONV layers and then continuing training**. If classification accuracy continues to improve (without overfitting), you may want to consider unfreezing more layers in the body."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 All in one"
      ],
      "metadata": {
        "id": "Vhi-zIbQz3rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Stage"
      ],
      "metadata": {
        "id": "kyaslrdp19zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "4dOSJpRn0YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the base model from the pre-trained model VGG16\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
        "                                         include_top=False,\n",
        "                                         weights='imagenet')"
      ],
      "metadata": {
        "id": "B9ivfcMjyzUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = tf.keras.applications.vgg16.preprocess_input(x)\n",
        "outputs = base_model(x, training=False)\n",
        "base_model = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "bMXbN2dDzOA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "sApxWDp26XNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "headModel = FCHeadNet.build(base_model, len(class_names), 256)"
      ],
      "metadata": {
        "id": "Pf-k7s7N0DhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=base_model.input, outputs=headModel)"
      ],
      "metadata": {
        "id": "P8qxtgOQ0a_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GEw0PD9MztGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "metadata": {
        "id": "5xE7kXMMX2mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in model.layers[4].layers:\n",
        "\tlayer.trainable = False"
      ],
      "metadata": {
        "id": "JPYQQGot0ohz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "xDFfnicDWPt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop is frequently used in situations where we need to quickly obtain\n",
        "# reasonable performance (as is the case when we are trying to “warm up” a set of FC layers).\n",
        "opt = RMSprop(learning_rate=0.0001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "tMHbXkii1MDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the head of the network for a few epochs (all other\n",
        "# layers are frozen) -- this will allow the new FC layers to\n",
        "# start to become initialized with actual \"learned\" values\n",
        "# versus pure random\n",
        "print(\"[INFO] training head...\")\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=validation_dataset,\n",
        "                    batch_size=32,\n",
        "                    epochs=3,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "_JTzCnrj1iG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Stage"
      ],
      "metadata": {
        "id": "yZz1RRmr2rlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the layers in the network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "metadata": {
        "id": "yJnc4nfSX9Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over the layers in the base model network and display them to the\n",
        "# console\n",
        "for (i, layer) in enumerate(model.layers[4].layers):\n",
        "\tprint(\"[INFO] {}\\t{}\".format(i, layer.__class__.__name__))"
      ],
      "metadata": {
        "id": "Glp3XooL59YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "sRM5m4p_YHK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that the head FC layers have been trained/initialized, lets\n",
        "# unfreeze the final set of CONV layers and make them trainable\n",
        "for layer in model.layers[4].layers[15:]:\n",
        "\tlayer.trainable = True"
      ],
      "metadata": {
        "id": "7v4963Wa6492"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "3K5DiQQkYMVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the changes to the model to take affect we need to recompile\n",
        "# the model, this time using SGD with a *very* small learning rate\n",
        "print(\"[INFO] re-compiling model...\")\n",
        "opt = SGD(learning_rate=0.0001)\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=opt,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "5SBHE_Oj7JGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model again, this time fine-tuning *both* the final set\n",
        "# of CONV layers along with our set of FC layers\n",
        "print(\"[INFO] training head...\")\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=validation_dataset,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "7RSFOc6Y7hWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sQqnorJ2Uk_"
      },
      "source": [
        "Additional accuracy can be obtained by performing more aggressive data augmentation and continually unfreezing more and more CONV blocks in VGG16. While fine-tuning is certainly more work than feature extraction, it also enables us to tune and modify the weights in our CNN to a particular dataset – something that feature extraction does not allow. Thus, when given enough training data, consider applying fine-tuning as you’ll likely obtain higher classification accuracy\n",
        "than simple feature extraction alone."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
        "\n",
        "ax.plot(np.arange(0, 10), history.history[\"loss\"], label=\"train_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 10), history.history[\"val_loss\"], label=\"val_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 10), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "ax.plot(np.arange(0, 10), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "ax.set_title(\"Training Loss and Accuracy\")\n",
        "ax.set_xlabel(\"Epoch #\")\n",
        "ax.set_ylabel(\"Loss/Accuracy\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WWeCOiS780CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = tuple(zip(*test_dataset))\n",
        "images = np.concatenate(images,axis=0)\n",
        "labels = np.concatenate(labels,axis=0)"
      ],
      "metadata": {
        "id": "MbZtvc9-9FSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy5R8lOV9FSt"
      },
      "outputs": [],
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(images, batch_size=32)\n",
        "print(classification_report(tf.argmax(labels, axis=1),\n",
        "                            tf.argmax(predictions, axis=1),\n",
        "                            target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGLdV6ag9FSt"
      },
      "outputs": [],
      "source": [
        "fig_confusion_matrix, ax = plt.subplots(1,1,figsize=(7,4))\n",
        "ConfusionMatrixDisplay(confusion_matrix(tf.argmax(predictions, axis=1),\n",
        "                                        tf.argmax(labels, axis=1)),\n",
        "                       display_labels=class_names).plot(values_format=\".0f\",ax=ax)\n",
        "\n",
        "ax.set_xlabel(\"True Label\")\n",
        "ax.set_ylabel(\"Predicted Label\")\n",
        "ax.grid(False)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ApYpc47MFOYi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4RpK9pawQzP"
      },
      "source": [
        "# Your First Image Classifier: Using CNN to Classify Images\n",
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbuzxq-b90j-"
      },
      "source": [
        "The purpose of this dataset is to correctly classify an image as containing a dog, cat, or panda.\n",
        "Containing only 3,000 images, the Animals dataset is meant to be another **introductory** dataset\n",
        "that we can quickly train a CNN model and obtain a comparative results with the previous KNN model.\n",
        "\n",
        "\n",
        "Let's take the following steps:\n",
        "\n",
        "1. Encoding target variable\n",
        "2. Training the CNN model\n",
        "3. Export the model and the encoder object\n",
        "\n",
        "<center><img width=\"900\" src=\"https://drive.google.com/uc?export=view&id=1haMB_Zt6Et9q9sPHxfuR4g3FT5QRXlTI\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApYpc47MFOYi"
      },
      "source": [
        "## Step 01: Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBka-lQFJW9"
      },
      "source": [
        "Start out by installing the experiment tracking library and setting up your free W&B account:\n",
        "\n",
        "\n",
        "*   **pip install wandb** – Install the W&B library\n",
        "*   **import wandb** – Import the wandb library\n",
        "*   **wandb login** – Login to your W&B account so you can log all your metrics in one place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWnFIWPuFXej"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a Python package for tracking the carbon emissions produced by various\n",
        "# kinds of computer programs, from straightforward algorithms to deep neural networks.\n",
        "!pip install codecarbon"
      ],
      "metadata": {
        "id": "NBqatao0jyp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcrOk6pURp50"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJaCNlDDRz6d"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from codecarbon import EmissionsTracker\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from wandb.keras import WandbCallback\n",
        "import os\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "7ee2tdx7Cnfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure logging\n",
        "# reference for a logging obj\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# set level of logging\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# create handlers\n",
        "c_handler = logging.StreamHandler()\n",
        "c_format = logging.Formatter(fmt=\"%(asctime)s %(message)s\",datefmt='%d-%m-%Y %H:%M:%S')\n",
        "c_handler.setFormatter(c_format)\n",
        "\n",
        "# add handler to the logger\n",
        "logger.handlers[0] = c_handler"
      ],
      "metadata": {
        "id": "Xq0zG_132yC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-XgvGlGx-n_"
      },
      "source": [
        "## Step 02 Basic configuration and download artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlLArYonw4pJ"
      },
      "outputs": [],
      "source": [
        "# since we are using Jupyter Notebooks we can replace our argument\n",
        "# parsing code with *hard coded* arguments and values\n",
        "args = {\n",
        "  \"project_name\": \"cnn_classifier\",\n",
        "  \"train_feature_artifact\": \"train_x:latest\",\n",
        "  \"train_target_artifact\": \"train_y:latest\",\n",
        "  \"val_feature_artifact\": \"val_x:latest\",\n",
        "  \"val_target_artifact\": \"val_y:latest\",\n",
        "  \"encoder\": \"target_encoder\",\n",
        "  \"inference_model\": \"model.h5\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3kCOus2wzbw"
      },
      "outputs": [],
      "source": [
        "# open the W&B project created in the Fetch step\n",
        "run = wandb.init(entity=\"ivanovitch-silva\",project=args[\"project_name\"], job_type=\"Train\")\n",
        "\n",
        "logger.info(\"Downloading the train and validation data\")\n",
        "# train x\n",
        "train_x_artifact = run.use_artifact(args[\"train_feature_artifact\"])\n",
        "train_x_path = train_x_artifact.file()\n",
        "\n",
        "# train y\n",
        "train_y_artifact = run.use_artifact(args[\"train_target_artifact\"])\n",
        "train_y_path = train_y_artifact.file()\n",
        "\n",
        "# validation x\n",
        "val_x_artifact = run.use_artifact(args[\"val_feature_artifact\"])\n",
        "val_x_path = val_x_artifact.file()\n",
        "\n",
        "# validation y\n",
        "val_y_artifact = run.use_artifact(args[\"val_target_artifact\"])\n",
        "val_y_path = val_y_artifact.file()\n",
        "\n",
        "# unpacking the artifacts\n",
        "train_x = joblib.load(train_x_path)\n",
        "train_y = joblib.load(train_y_path)\n",
        "val_x = joblib.load(val_x_path)\n",
        "val_y = joblib.load(val_y_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Train x: {}\".format(train_x.shape))\n",
        "logger.info(\"Train y: {}\".format(train_y.shape))\n",
        "logger.info(\"Validation x: {}\".format(val_x.shape))\n",
        "logger.info(\"Validation y: {}\".format(val_y.shape))"
      ],
      "metadata": {
        "id": "81mqk7Z2DMOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[3]"
      ],
      "metadata": {
        "id": "MdPzrKx0yeUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[3]"
      ],
      "metadata": {
        "id": "oKpihG7XyjSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 03: Encoder the target variable"
      ],
      "metadata": {
        "id": "68hjvjv-JzEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the labels as Binarizers\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "# take care not to produce data lakeage\n",
        "train_y = lb.fit_transform(train_y)\n",
        "val_y = lb.transform(val_y)"
      ],
      "metadata": {
        "id": "Ht06smfRwngl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lb.classes_"
      ],
      "metadata": {
        "id": "BVYLnb1bKUu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y[4]"
      ],
      "metadata": {
        "id": "DuTIUfm2KRuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_y[3]"
      ],
      "metadata": {
        "id": "Ln7wJ93FKZex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 04: Model definition"
      ],
      "metadata": {
        "id": "ggiDYSthKfMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source code based on **Rosebrock, Adrian. Deep Learning For Computer vision with Python, 2019** [link](https://pyimagesearch.com/deep-learning-computer-vision-python-book/)"
      ],
      "metadata": {
        "id": "b0pYGw2LK0Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowNet:\n",
        "\t@staticmethod\n",
        "\tdef build(width, height, depth, classes):\n",
        "\t\t# initialize the model along with the input shape to be\n",
        "\t\t# \"channels last\"\n",
        "\t\tmodel = Sequential()\n",
        "\t\tinputShape = (height, width, depth)\n",
        "\n",
        "\t\t# if we are using \"channels first\", update the input shape\n",
        "\t\tif K.image_data_format() == \"channels_first\":\n",
        "\t\t\tinputShape = (depth, height, width)\n",
        "\n",
        "\t\t# define the first (and only) CONV => RELU layer\n",
        "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\n",
        "\t\t# softmax classifier\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(classes))\n",
        "\t\tmodel.add(Activation(\"softmax\"))\n",
        "\n",
        "\t\t# return the constructed network architecture\n",
        "\t\treturn model"
      ],
      "metadata": {
        "id": "BBkEAue-KwQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model object\n",
        "model = ShallowNet.build(32,32,3,3)\n",
        "\n",
        "# summarize layers\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "aht6tkQILWaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "32768*3 + 3"
      ],
      "metadata": {
        "id": "N6cunC8y4TBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "32*32*32"
      ],
      "metadata": {
        "id": "b9A1Sk3-4F5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = WX + b"
      ],
      "metadata": {
        "id": "L5k0O5dJ3z9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3*9*32 + 32"
      ],
      "metadata": {
        "id": "6vg5Ko2g3ahf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 05: Training"
      ],
      "metadata": {
        "id": "T9hAAmxudSAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create codecarbon tracker\n",
        "# codecarbon is too much verbose, change the log level for more info\n",
        "tracker = EmissionsTracker(log_level=\"critical\")\n",
        "tracker.start()\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = SGD(learning_rate=0.005)\n",
        "model = ShallowNet.build(width=32, height=32, depth=3, classes=3)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "# train the network\n",
        "print(\"[INFO] training network...\")\n",
        "history = model.fit(train_x, train_y, \n",
        "              validation_data=(val_x, val_y),\n",
        "              batch_size=32, \n",
        "              epochs=100, \n",
        "              verbose=0,\n",
        "              callbacks=[wandb.keras.WandbCallback(save_model=False,\n",
        "                                                   compute_flops=True)]\n",
        "          )\n",
        "\n",
        "# get co2 emissions from tracker\n",
        "# \"CO2 emission (in Kg)\"\n",
        "emissions = tracker.stop()"
      ],
      "metadata": {
        "id": "9j_awuNTdh1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 06: Evaluation Metrics"
      ],
      "metadata": {
        "id": "JOgzx5LhEPhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "fig, ax = plt.subplots(1,1,figsize=(10,8))\n",
        "\n",
        "ax.plot(np.arange(0, 100), history.history[\"loss\"], label=\"train_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 100), history.history[\"val_loss\"], label=\"val_loss\",linestyle='--')\n",
        "ax.plot(np.arange(0, 100), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "ax.plot(np.arange(0, 100), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "ax.set_title(\"Training Loss and Accuracy\")\n",
        "ax.set_xlabel(\"Epoch #\")\n",
        "ax.set_ylabel(\"Loss/Accuracy\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxdbI3o4kXqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] {} kWh of electricity used since the begining\".format(tracker.final_emissions_data.energy_consumed))\n",
        "print(\"[INFO] Energy consumed for RAM: {} kWh\".format(tracker.final_emissions_data.ram_energy))\n",
        "print(\"[INFO] Energy consumed for all GPU: {} kWh\".format(tracker.final_emissions_data.gpu_energy))\n",
        "print(\"[INFO] Energy consumed for all CPU: {} kWh\".format(tracker.final_emissions_data.cpu_energy))\n",
        "print(\"[INFO] CO2 emission {}(in Kg)\".format(tracker.final_emissions_data.emissions))"
      ],
      "metadata": {
        "id": "pb_roodzuH2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(val_x, batch_size=32)\n",
        "print(classification_report(val_y.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1),\n",
        "                            target_names=lb.classes_))"
      ],
      "metadata": {
        "id": "9e5ozfDaAo6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_confusion_matrix, ax = plt.subplots(1,1,figsize=(7,4))\n",
        "ConfusionMatrixDisplay(confusion_matrix(predictions.argmax(axis=1),\n",
        "                                        val_y.argmax(axis=1)),\n",
        "                       display_labels=lb.classes_).plot(values_format=\".0f\",ax=ax)\n",
        "\n",
        "ax.set_xlabel(\"True Label\")\n",
        "ax.set_ylabel(\"Predicted Label\")\n",
        "ax.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-63tXNhAFwQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading figures\n",
        "logger.info(\"Uploading figures\")\n",
        "run.log(\n",
        "    {\n",
        "        \"confusion_matrix\": wandb.Image(fig_confusion_matrix),\n",
        "        # \"other_figure\": wandb.Image(other_fig)\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "bi7ugT4pHA_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "logger.info(\"Validation Evaluation metrics\")\n",
        "fbeta = fbeta_score(val_y.argmax(axis=1), \n",
        "                    predictions.argmax(axis=1), \n",
        "                    beta=1, zero_division=1,average='weighted')\n",
        "precision = precision_score(val_y.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1),\n",
        "                            zero_division=1,average='weighted')\n",
        "recall = recall_score(val_y.argmax(axis=1),\n",
        "                      predictions.argmax(axis=1),\n",
        "                      zero_division=1,average='weighted')\n",
        "acc = accuracy_score(val_y.argmax(axis=1),\n",
        "                     predictions.argmax(axis=1))\n",
        "\n",
        "logger.info(\"Validation Accuracy: {}\".format(acc))\n",
        "logger.info(\"Validation Precision: {}\".format(precision))\n",
        "logger.info(\"Validation Recall: {}\".format(recall))\n",
        "logger.info(\"Validation F1: {}\".format(fbeta))\n",
        "\n",
        "run.summary[\"Acc\"] = acc\n",
        "run.summary[\"Precision\"] = precision\n",
        "run.summary[\"Recall\"] = recall\n",
        "run.summary[\"F1\"] = fbeta\n",
        "# number of parameters\n",
        "run.summary[\"Count_Params\"] = model.count_params()\n",
        "# energy unit is kWh\n",
        "run.summary[\"Energy_Consumed\"] = tracker.final_emissions_data.energy_consumed\n",
        "run.summary[\"Energy_RAM\"] = tracker.final_emissions_data.ram_energy\n",
        "run.summary[\"Energy_GPU\"] = tracker.final_emissions_data.gpu_energy\n",
        "run.summary[\"Energy_CPU\"] = tracker.final_emissions_data.cpu_energy\n",
        "# kg\n",
        "run.summary[\"CO2_Emissions\"] = tracker.final_emissions_data.emissions"
      ],
      "metadata": {
        "id": "9iOE5gXOB-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"Dumping the model and encoder artifacts to the disk\")\n",
        "\n",
        "# Save the artifacts using joblib\n",
        "joblib.dump(lb, args[\"encoder\"])\n",
        "model.save(os.path.join(wandb.run.dir, args[\"inference_model\"]))"
      ],
      "metadata": {
        "id": "rnWPb4pTxW0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder artifact\n",
        "artifact = wandb.Artifact(args[\"encoder\"],\n",
        "                          type=\"INFERENCE_MODEL\",\n",
        "                          description=\"A json file representing the target encoder\"\n",
        "                          )\n",
        "\n",
        "logger.info(\"Logging the target encoder artifact\")\n",
        "artifact.add_file(args[\"encoder\"])\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "dI3qHb2yy98B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference model artifact\n",
        "artifact = wandb.Artifact(args[\"inference_model\"],\n",
        "                          type=\"INFERENCE_MODEL\",\n",
        "                          description=\"A json file representing the inference model\"\n",
        "                          )\n",
        "\n",
        "logger.info(\"Logging the inference model artifact\")\n",
        "artifact.add_file(os.path.join(wandb.run.dir, args[\"inference_model\"]))\n",
        "run.log_artifact(artifact)"
      ],
      "metadata": {
        "id": "VvgTyWkt05sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "wpA5T4NR1Fpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can we improve this model?**"
      ],
      "metadata": {
        "id": "pE_6EmjCrDlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Data Augmentation [Link](https://colab.research.google.com/drive/1S8SJvH4bqhPvurG4gjh3-t-XulX4S8JX#scrollTo=me4Jr5IhaT0j)\n",
        "- Batch Normalization\n",
        "- Dropout\n",
        "- Add more CNN layers\n",
        "- Add more hidden layers in the head"
      ],
      "metadata": {
        "id": "BLUjdXqVLdvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 07: Sweep (hyperparameter tuning)"
      ],
      "metadata": {
        "id": "CH2gJjHWfn_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sweep setup"
      ],
      "metadata": {
        "id": "7xQj4JD08vAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ℹ️ [Reference](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration)"
      ],
      "metadata": {
        "id": "kBfnPmHARHl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sweep configuration structure**\n",
        "\n",
        "Sweep configurations are nested; keys can have, as their values, further keys. The top-level keys are listed and briefly described below, and then detailed in the following section.\n",
        "\n",
        "| Top-Level Key | Description                                         |\n",
        "|---------------|-----------------------------------------------------|\n",
        "| **program**       | (required) Training script to run.                  |\n",
        "| **method**        | (required) Specify the <br>search strategy.         |\n",
        "| **parameters**    | (required) Specify <br>parameters bounds to search. |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Search type methods**\n",
        "\n",
        "The following list describes hyperparameter search methods. Specify the search strategy with the **method**:\n",
        "\n",
        "- **grid**  – Iterate over every combination of hyperparameter values. Can be computationally costly.\n",
        "- **random**  – Choose a random set of hyperparameter values on each iteration based on provided distributions.\n",
        "- **bayes** – Create a probabilistic model of a metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. \n",
        "<br>\n",
        "\n",
        "**Metric**\n",
        "\n",
        "Describes the metric to optimize. This metric should be logged **explicitly** to W&B by your training script.\n",
        "\n",
        "| Key    | Description |\n",
        "|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **name**   | Name of the metric to optimize.|\n",
        "| **goal**   | Either minimize or  maximize (Default is <br>minimize.|\n",
        "| **target** | Goal value for the metric you're optimizing. <br>When any run in the sweep achieves that target value,<br> the sweep's state will be set to finished. <br>This means all agents with active runs will <br>finish those jobs, but no new runs will <br>be launched in the sweep. |"
      ],
      "metadata": {
        "id": "F3ustRzogBbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the sweep \n",
        "sweep_config = {\n",
        "    'method': 'random', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'conv_layer': {\n",
        "            'max': 3,\n",
        "            'min': 1,\n",
        "            'distribution': 'int_uniform',\n",
        "        },\n",
        "        'hidden_layer': {\n",
        "            'values': [0,1,2,3]\n",
        "        },\n",
        "        'learn_rate': {\n",
        "            'values': [0.01,0.001,0.005],  \n",
        "        },\n",
        "        'epoch': {\n",
        "            'values': [100,200]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [32,64]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "RrHacnrZRJaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️⚠️⚠️\n",
        "\n",
        "> Please, you must run again the following steps and command before go ahead to step 7:\n",
        "- **Step 02** (basic configuration and download artifacts) and \n",
        "- **Step 03** (encoder the target variable)\n",
        "- **run.finish()**"
      ],
      "metadata": {
        "id": "D9U9LRVe59sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new sweep\n",
        "# Arguments:\n",
        "#     – sweep_config: the sweep config dictionary defined above\n",
        "#     – entity: Set the username for the sweep\n",
        "#     – project: Set the project name for the sweep\n",
        "sweep_id = wandb.sweep(sweep_config,\n",
        "                       entity=\"ivanovitch-silva\",\n",
        "                       project=args[\"project_name\"])"
      ],
      "metadata": {
        "id": "40OA8KI8cpi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapt the model"
      ],
      "metadata": {
        "id": "NAG8lJo99BB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowNetAdapt:\n",
        "  @staticmethod\n",
        "  def build(width, height, depth, classes, config):\n",
        "    '''\n",
        "      width, height, depth: dimensions of the image\n",
        "      classes: number of targets\n",
        "      config: variable used to configure the sweep\n",
        "    '''\n",
        "    # initialize the model along with the input shape to be\n",
        "    # \"channels last\"\n",
        "    model = Sequential()\n",
        "    inputShape = (height, width, depth)\n",
        "\n",
        "    # if we are using \"channels first\", update the input shape\n",
        "    if K.image_data_format() == \"channels_first\":\n",
        "      inputShape = (depth, height, width)\n",
        "\n",
        "    for i in range(config.conv_layer):\n",
        "      # define a convolution layer followed by a relu activation \n",
        "      # CONV => RELU layer\n",
        "      model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    # add a flatten layer\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # add hidden layers followed by a relu activation\n",
        "    for j in range(config.hidden_layer):\n",
        "      model.add(Dense(10,activation=\"relu\"))\n",
        "\n",
        "    # softmax classifier\n",
        "    model.add(Dense(classes))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    # return the constructed network architecture\n",
        "    return model"
      ],
      "metadata": {
        "id": "O4I6p1tK9K8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "T9iLZFEuJLob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init() as run:\n",
        "\n",
        "      # create codecarbon tracker\n",
        "      # codecarbon is too much verbose, change the log level for more info\n",
        "      tracker = EmissionsTracker(log_level=\"critical\")\n",
        "      tracker.start()\n",
        "\n",
        "      # initialize the optimizer and model\n",
        "      print(\"[INFO] compiling model...\")\n",
        "      opt = SGD(learning_rate=run.config.learn_rate)\n",
        "      model = ShallowNetAdapt.build(width=32, \n",
        "                               height=32,\n",
        "                               depth=3, \n",
        "                               classes=3,\n",
        "                               config=run.config)\n",
        "      model.compile(loss=\"categorical_crossentropy\", \n",
        "                    optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "      # train the network\n",
        "      print(\"[INFO] training network...\")\n",
        "      history = model.fit(train_x, train_y, \n",
        "                    validation_data=(val_x, val_y),\n",
        "                    batch_size=run.config.batch_size, \n",
        "                    epochs=run.config.epoch, \n",
        "                    verbose=0,\n",
        "                    callbacks=[wandb.keras.WandbCallback(save_model=False,\n",
        "                                                        compute_flops=True)]\n",
        "                )\n",
        "\n",
        "      # get co2 emissions from tracker\n",
        "      # \"CO2 emission (in Kg)\"\n",
        "      emissions = tracker.stop()\n",
        "\n",
        "      # make predictions\n",
        "      predictions = model.predict(val_x, batch_size=run.config.batch_size)\n",
        "\n",
        "      # Evaluation Metrics\n",
        "      logger.info(\"Validation Evaluation metrics\")\n",
        "      fbeta = fbeta_score(val_y.argmax(axis=1), \n",
        "                          predictions.argmax(axis=1), \n",
        "                          beta=1, zero_division=1,average='weighted')\n",
        "      precision = precision_score(val_y.argmax(axis=1),\n",
        "                                  predictions.argmax(axis=1),\n",
        "                                  zero_division=1,average='weighted')\n",
        "      recall = recall_score(val_y.argmax(axis=1),\n",
        "                            predictions.argmax(axis=1),\n",
        "                            zero_division=1,average='weighted')\n",
        "      acc = accuracy_score(val_y.argmax(axis=1),\n",
        "                          predictions.argmax(axis=1))\n",
        "\n",
        "      logger.info(\"Validation Accuracy: {}\".format(acc))\n",
        "      logger.info(\"Validation Precision: {}\".format(precision))\n",
        "      logger.info(\"Validation Recall: {}\".format(recall))\n",
        "      logger.info(\"Validation F1: {}\".format(fbeta))\n",
        "\n",
        "      run.summary[\"Acc\"] = acc\n",
        "      run.summary[\"Precision\"] = precision\n",
        "      run.summary[\"Recall\"] = recall\n",
        "      run.summary[\"F1\"] = fbeta\n",
        "      # number of parameters\n",
        "      run.summary[\"Count_Params\"] = model.count_params()\n",
        "      # energy unit is kWh\n",
        "      run.summary[\"Energy_Consumed\"] = tracker.final_emissions_data.energy_consumed\n",
        "      run.summary[\"Energy_RAM\"] = tracker.final_emissions_data.ram_energy\n",
        "      run.summary[\"Energy_GPU\"] = tracker.final_emissions_data.gpu_energy\n",
        "      run.summary[\"Energy_CPU\"] = tracker.final_emissions_data.cpu_energy\n",
        "      # kg\n",
        "      run.summary[\"CO2_Emissions\"] = tracker.final_emissions_data.emissions\n"
      ],
      "metadata": {
        "id": "6JrqO2Z8c23K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a new sweep\n",
        "# Arguments:\n",
        "#     – sweep_id: the sweep_id to run - this was returned above by wandb.sweep()\n",
        "#     – function: function that defines your model architecture and trains it\n",
        "wandb.agent(sweep_id = sweep_id, function=train,count=3)"
      ],
      "metadata": {
        "id": "sYJSjWcsja86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.finish()"
      ],
      "metadata": {
        "id": "JIzy5nxzn0Zq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
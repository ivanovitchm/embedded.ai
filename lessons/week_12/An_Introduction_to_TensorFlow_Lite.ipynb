{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preface"
      ],
      "metadata": {
        "id": "O7a9lvVPBil3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> TensorFlow Lite it’s not\n",
        "a framework for training models, but a supplementary set of tools designed to meet\n",
        "all the constraints of mobile and embedded systems. *Laurence Moroney, AI and ML for Coders*. \n",
        "\n",
        "TF-Lite should broadly be seen as two main things: **a converter** that takes your TensorFlow\n",
        "model and converts it to the .tflite format, shrinking and optimizing it, and a **suite of\n",
        "interpreters** for various runtimes.\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1XIDqEiOl4F33y5mkDQKr93oShFM2qAnS\"></center>\n",
        "\n",
        "\n",
        "Note that not every operation (or “op”) in TensorFlow is presently supported in\n",
        "TensorFlow Lite or the TensorFlow Lite converter. You may encounter this issue\n",
        "when [converting](https://www.tensorflow.org/lite/models/convert/convert_models) models, and it’s always a good idea to check the [documentation](https://www.tensorflow.org/lite/guide/ops_compatibility) for\n",
        "details. \n",
        "\n"
      ],
      "metadata": {
        "id": "NihH_FpHBuXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough: Creating and Converting a Model to TensorFlow Lite"
      ],
      "metadata": {
        "id": "lARySBvkZcbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll begin with a step-by-step walkthrough showing how to create a simple model with TensorFlow, convert it to the TensorFlow Lite format, and then use the Tensor‐\n",
        "Flow Lite interpreter. \n",
        "\n",
        "For the sake of understanding, we will evaluate a very simple TensorFlow model that learned the relationship between two sets of numbers that ended up as:\n",
        "\n",
        "$\n",
        "y = 2x – 1\n",
        "$"
      ],
      "metadata": {
        "id": "GZRLXDa8ZmvL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6k2Pg0gTYB8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAwdHf6ySQGt"
      },
      "source": [
        "layer_0 = Dense(units=1, input_shape=[1])\n",
        "model = Sequential([layer_0])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500, verbose=0)\n",
        "\n",
        "print(model.predict([10.0]))\n",
        "# get_weights() return the weights and bias\n",
        "print(\"Here is what I learned: {}\".format(layer_0.get_weights()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "Nim1GwGVu86O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorFlow Lite converter works on a number of [different file formats](https://www.tensorflow.org/lite/models/convert), including\n",
        "SavedModel (preferred) and the Keras H5 format. For this exercise we’ll use\n",
        "SavedModel. Once you have the saved model, you can convert it using the TensorFlow Lite\n",
        "converter."
      ],
      "metadata": {
        "id": "DpX9O9wdwPcw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOs_IDM6ToaM"
      },
      "source": [
        "# Save the model\n",
        "export_dir = 'saved_model/1'\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert and Save the Mode"
      ],
      "metadata": {
        "id": "0vL6hR6dxGbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorFlow Lite converter is in the ``tf.lite`` package. You can call it to convert a\n",
        "saved model by first invoking it with the ``from_saved_model`` method, passing it the\n",
        "directory containing the saved model, and then invoking its convert method."
      ],
      "metadata": {
        "id": "aE-mAEiHxN4X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWSlkprhTsWE"
      },
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then save out the new ``.tflite`` model using ``pathlib``."
      ],
      "metadata": {
        "id": "6hHurCK1xcJX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsaEjJfrTujk"
      },
      "source": [
        "import pathlib\n",
        "tflite_model_file = pathlib.Path('model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you have a ``.tflite`` file that you can use in any of the interpreter environments. Let’s use the Python-based\n",
        "interpreter so you can run it in Colab. This same interpreter can be used in embedded Linux environments like a Raspberry Pi!"
      ],
      "metadata": {
        "id": "H0UEjxDSxykN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load the TFLite Model and Allocate Tensors"
      ],
      "metadata": {
        "id": "54dSi-bKyB92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to **load the model into the interpreter**, **allocate tensors** that will be\n",
        "used for inputting data to the model for prediction, and then read the predictions that\n",
        "the model outputs. \n",
        "\n",
        "> This is where using ``TensorFlow Lite``, from a programmer’s perspective, greatly differs from using TensorFlow. \n",
        "\n",
        "With TensorFlow you can just say\n",
        "``model.predict(something)`` and get the results, but because TensorFlow Lite won’t\n",
        "have many of the dependencies that TensorFlow does, particularly in non-Python\n",
        "environments, you now have to get a bit more low-level and deal with the input and\n",
        "output tensors, formatting your data to fit them and parsing the output in a way that makes sense for your device.\n",
        "\n",
        "First, load the model and allocate the tensors:"
      ],
      "metadata": {
        "id": "Gv_UgP1LyZEw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fseX4pkhTzS0"
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can get the input and output details from the model, so you can begin to\n",
        "understand what data format it expects, and what data format it will provide back to you:"
      ],
      "metadata": {
        "id": "LbvgDVRHy41U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "pprint(input_details)"
      ],
      "metadata": {
        "id": "sw3vgYkoyyOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First, let’s inspect the input parameter**. Note the shape setting, which is an array of\n",
        "type ``[1,1]``. Also note the class, which is ``numpy.float32``. These settings will dictate the shape of the input data and its format.\n",
        "\n",
        "So, in order to format the input data, you’ll need to use code like this to define the\n",
        "input array shape and type if you want to predict the ``y`` for ``x=10.0``:"
      ],
      "metadata": {
        "id": "xttawWGC0tVv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Ij8_BvU0KV"
      },
      "source": [
        "to_predict = np.array([[10.0]], dtype=np.float32)\n",
        "print(to_predict)\n",
        "print(to_predict.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The double brackets around the 10.0 can cause a little confusion—the mnemonic I\n",
        "use for the ``array[1,1]`` here is to say that there is 1 list, giving us the first set of ``[]``,\n",
        "and that list contains just 1 value, which is ``[10.0]``, thus giving ``[[10.0]]``. It can also\n",
        "be confusing that the shape is defined as ``dtype=int32``, whereas you’re using\n",
        "``numpy.float32``. The dtype parameter is the data type defining the shape, not the\n",
        "contents of the list that is encapsulated in that shape. For that, you’ll use the class."
      ],
      "metadata": {
        "id": "qAim1vk41bOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output details are very similar, and what you want to keep an eye on here is the\n",
        "shape. Because it’s also an array of type ``[1,1]``, you can expect the answer to be ``[[y]]``\n",
        "in much the same way as the input was ``[[x]]``:"
      ],
      "metadata": {
        "id": "Me-WNY8J2QyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(output_details)"
      ],
      "metadata": {
        "id": "QPC18hfO2NYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform the Prediction"
      ],
      "metadata": {
        "id": "fGUe1Mu12emM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the interpreter to do the prediction, you set the input tensor with the value to predict, telling it what input value to use:"
      ],
      "metadata": {
        "id": "hlpqesAe2jLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter.set_tensor(input_details[0]['index'], to_predict)\n",
        "interpreter.invoke()"
      ],
      "metadata": {
        "id": "MiQM9yMU0sZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input tensor is specified using the index of the array of input details. In this case\n",
        "you have a very simple model that has only a single input option, so it’s\n",
        "``input_details[0]``, and you’ll address it at the index. Input details item 0 has only\n",
        "one index, indexed at 0, and it expects a shape of ``[1,1]`` as defined earlier. So, you put the ``to_predict`` value in there. Then you invoke the interpreter with the ``invoke``\n",
        "method."
      ],
      "metadata": {
        "id": "Co1a-Tvk3ECn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then read the prediction by calling ``get_tensor`` and supplying it with the details of the tensor you want to read:"
      ],
      "metadata": {
        "id": "YGp_9Awm4_DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(tflite_results)"
      ],
      "metadata": {
        "id": "d9V4igO32n33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that this is a very simple example, let’s look at something a little more complex next—using transfer learning on a well-known image classification model, and then converting that for TensorFlow Lite. From there we’ll also be able to better explore the impacts of optimizing and quantizing the model."
      ],
      "metadata": {
        "id": "5-U7veD25Lln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Visualization"
      ],
      "metadata": {
        "id": "N0XMQPjL5ie0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use [Netron](https://netron.app/) to visualize the model. Netron is a viewer for neural network, deep learning and machine learning models. Just load the file ``model.tflite``on it. Note that convertion of TF model into a TF-Lite not considered quantization (all weights and bias are ``float32``).\n",
        "\n",
        "\n",
        "<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1pasZ7O6NFK8OVxNBg_EXXGQhXl_qZPgs\"></center>\n"
      ],
      "metadata": {
        "id": "CDx-06ZqCbcX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEJSvtvlCto-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}